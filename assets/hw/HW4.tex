\documentclass[12pt,letterpaper, onecolumn]{exam}
\usepackage[lmargin=71pt, tmargin=1.2in]{geometry}  %For centering solution box
\lhead{STAT 479\\}
\rhead{HW 4\\}
\thispagestyle{empty}   %For removing header/footer from page 1
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{tikz-cd}
\titlespacing{\section}{0pt}{*0.2}{*0.2}


\usepackage{tikz}
\newcommand*\circled[1]{
\tikz[baseline=(char.base)]{\node[shape=circle,draw,inner sep=1pt] (char) {#1};}
}
            
 
% xun
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Cbs}{\boldsymbol{C}}
\newcommand{\Sbs}{\boldsymbol{S}}
\newcommand{\Pa}{\text{Pa}}
\newcommand{\De}{\text{De}}
\newcommand{\Nd}{\text{Nd}}
            


\begin{document}

\begingroup  
    \centering
    \LARGE STAT 479: Homework 4\\[0.5em]
    \large Due: 11:59PM Mar 8, 2025 by Canvas\\[0.5em]
\endgroup
\rule{\textwidth}{0.4pt}
\pointsdroppedatright   %Self-explanatory
\printanswers
\renewcommand{\solutiontitle}{\noindent\textbf{Answer:}\enspace}

\vspace{-20pt}
\begin{questions}
\vspace{-10pt}


\question[20 points] \textbf{MLE in Bayesian Networks}\droppoints

Consider the following Bayesian Network with four binary variables:

\[
X_1 \rightarrow X_2, \quad X_1 \rightarrow X_3, \quad X_2 \rightarrow X_4, \quad X_3 \rightarrow X_4
\]

The dataset consists of:

\begin{center}
\begin{tabular}{cccc|c}
\toprule
$X_1$ & $X_2$ & $X_3$ & $X_4$ & \textbf{Count} \\
\midrule
0 & 0 & 0 & 0 & 30 \\
0 & 0 & 0 & 1 & 10 \\
0 & 0 & 1 & 0 & 20 \\
0 & 0 & 1 & 1 & 20 \\
0 & 1 & 0 & 0 & 25 \\
0 & 1 & 0 & 1 & 15 \\
0 & 1 & 1 & 0 & 35 \\
0 & 1 & 1 & 1 & 15 \\
1 & 0 & 0 & 0 & 20 \\
1 & 0 & 0 & 1 & 10 \\
1 & 0 & 1 & 0 & 25 \\
1 & 0 & 1 & 1 & 15 \\
1 & 1 & 0 & 0 & 30 \\
1 & 1 & 0 & 1 & 20 \\
1 & 1 & 1 & 0 & 40 \\
1 & 1 & 1 & 1 & 30 \\
\bottomrule
\end{tabular}
\end{center}

\begin{parts}
\part Compute the MLE for \( P(X_2=1 | X_1=0) \). Round to the nearest tenth.

\begin{choices}
    \choice \( 0.4 \)
    \choice \( 0.5 \)
    \choice \( 0.6 \)
    \choice \( 0.7 \)
\end{choices}

\part Compute the MLE for \( P(X_4=1 | X_2=1, X_3=0) \). Round to the nearest tenth.

\begin{choices}
    \choice \( 0.3 \)
    \choice \( 0.4 \)
    \choice \( 0.5 \)
    \choice \( 0.6 \)
\end{choices}

\part The variable \( X_4 \) is a collider. What happens when we condition on \( X_4 \)?

\begin{choices}
    \choice It blocks the path between \( X_2 \) and \( X_3 \), making them independent.
    \choice It introduces a correlation between \( X_2 \) and \( X_3 \).
    \choice It has no effect unless we also condition on \( X_1 \).
    \choice It enforces complete independence among all variables.
\end{choices}

\part After fitting our estimate of \( P(X_2 | X_1) \), someone informs us about an unobserved variable \( L \) that influences both \( X_2 \) and \( X_3 \), but we know we cannot observe \( L \). What should we do to our estimate?

\begin{choices}
    \choice Keep the estimate unchanged, since we already conditioned on \( X_1 \).
    \choice Acknowledge potential bias and use sensitivity analysis to estimate the possible impact of \( L \).
    \choice Widen our confidence intervals to ensure the true value is captured.
    \choice Ignore \( L \) since unobserved variables do not affect MLE estimates.
\end{choices}
\end{parts}

\begin{solution}
    \begin{parts}
        \part 
        \part 
        \part
        \part
    \end{parts}
\end{solution}


\question[20 points] \textbf{MRFs} \droppoints

Consider a Markov Random Field (MRF) with the following structure:
\begin{itemize}
    \item Nodes: \(X_1, X_2, X_3, X_4\)
    \item Edges: \((X_1, X_2), (X_2, X_3), (X_3, X_4), (X_4, X_1), (X_1, X_3)\)
    \item The joint probability distribution is given by:
    \[
    P(X_1, X_2, X_3, X_4) \propto \exp \left( \sum_{(i,j) \in E} \theta_{ij} X_i X_j \right)
    \]
\end{itemize}

\begin{parts}
\part Which of the following is a correct factorization of this MRF?
\begin{choices}
    \choice \( P(X_1, X_2, X_3, X_4) = \phi(X_1, X_2) \phi(X_2, X_3) \phi(X_3, X_4) \phi(X_4, X_1) \)
    \choice \( P(X_1, X_2, X_3, X_4) = \phi(X_1, X_2, X_3) \phi(X_3, X_4) \)
    \choice \( P(X_1, X_2, X_3, X_4) = \phi(X_1, X_2) \phi(X_2, X_3) \phi(X_3, X_4) \phi(X_4, X_1) \phi(X_1, X_3) \)
    \choice \( P(X_1, X_2, X_3, X_4) = \phi(X_1, X_2, X_3, X_4) \)
\end{choices}

\part Why is computing the partition function \(Z\) difficult for large MRFs?
\begin{choices}
    \choice The partition function requires summing over an exponential number of terms.
    \choice The partition function depends on the parameters \(\theta_{ij}\), which are unknown.
    \choice The partition function does not exist for undirected graphical models.
    \choice The partition function is always equal to 1.
\end{choices}

\part Removing edge \((X_1, X_3)\) changes which independence property?

\begin{choices}
    \choice \( X_1 \perp X_3 \mid \{X_2, X_4\} \)
    \choice \( X_1 \perp X_3 \mid X_2 \)
    \choice \( X_1 \perp X_4 \mid X_2 \)
    \choice \( X_1 \perp X_2 \mid X_4 \)
\end{choices}

% \part Consider a Gaussian graphical model where \( X = \{X_1, ..., X_d\} \) follows a joint Gaussian distribution:
% \[
% X \sim \mathcal{N}(\mu, \Lambda^{-1})
% \]
% where \( \Lambda \) is the precision matrix. Let \( X_j, X_k \) be two nodes, and let \( Z = \{X_i \mid i \notin \{j,k\} \} \) be the remaining nodes.  

% Which of the following best explains why \( X_j \perp X_k \mid Z \) if and only if \( \Lambda_{jk} = 0 \)?  

% \begin{choices}
%     \choice The conditional covariance of \( X_j \) and \( X_k \) is zero if and only if \( \Lambda_{jk} = 0 \), ensuring conditional independence.  
%     \choice The inverse of \( \Lambda \) (the covariance matrix) has a zero entry at position \( (j,k) \), meaning \( X_j \) and \( X_k \) are uncorrelated.  
%     \choice Zero entries in \( \Lambda \) correspond to missing edges in the Gaussian graphical model.  
%     \choice All of the above.  
% \end{choices}

\part A researcher is using Graphical Lasso to learn the structure of a Markov Random Field (MRF) from data. However, they observe that small changes in the regularization parameter result in large differences in the learned graph structure, with many edges appearing or disappearing unpredictably.

Which of the following is the most likely reason for this instability?

\begin{choices}
\choice The sample size is too small relative to the number of variables, leading to an unstable covariance estimate.
\choice The true MRF is not connected, so regularization causes disjoint components to form.
\choice Graphical Lasso is not a consistent estimator and always leads to instability.
\choice The data is non-Gaussian, violating the assumptions of Graphical Lasso.
\end{choices}
\end{parts}

\begin{solution}
    \begin{parts}
        \part 
        \part 
        \part
        \part
    \end{parts}
\end{solution}

\clearpage
\question[20 points]\textbf{Gaussian Graphical Models â€“ Step-by-Step Proof}\droppoints

We consider an undirected graphical model where a set of random variables \( X = \{X_1, ..., X_d\} \) follows a joint Gaussian distribution:

\[
X \sim \mathcal{N}(\mu, \Sigma)
\]

where \( \Sigma \) is the \textbf{covariance matrix} (\( \Sigma \in \mathbb{S}^{++} \)), and its inverse \( \Theta = \Sigma^{-1} \) is the \textbf{precision matrix}. Given two nodes \( X_j, X_k \), and the remaining variables \( Z = \{X_i \mid i \notin \{j,k\}\} \), we want to show:

\[
X_j \perp X_k \mid Z \quad \text{if and only if} \quad \Theta_{jk} = 0.
\]

\begin{parts}
\part \textbf{Step 1: Conditional Independence in Gaussian Graphical Models}

The joint probability density function of a multivariate Gaussian is:

\[
p(X) \propto \exp \left( -\frac{1}{2} X^T \Theta X \right)
\]

From this, we can see that the entries of the \textbf{precision matrix} \( \Theta \) describe:

\begin{choices}
    \choice The marginal variances of \( X \).
    \choice The correlations between variables.
    \choice The structure of the conditional dependencies between variables.
    \choice The eigenvalues of the covariance matrix.
\end{choices}

\part \textbf{Step 2: Identifying the Relevant Conditional Distribution}

To determine whether \( X_j \) and \( X_k \) are conditionally independent given \( Z \), we need to examine:

\begin{choices}
    \choice The marginal covariance matrix of \( X_j \) and \( X_k \).
    \choice The conditional precision matrix of \( X_j \) and \( X_k \) given \( Z \).
    \choice The determinant of \( \Theta \).
    \choice The sum of all precision matrix entries.
\end{choices}

\part \textbf{Step 3: Expressing the Conditional Distribution}

When conditioning on \( Z \), the precision matrix for \( X_j, X_k \mid Z \) is given by:
\[
\Theta_{(j,k) \mid Z} = \Theta_{\{j,k\}, \{j,k\}} - \Theta_{\{j,k\}, Z} \Theta_{Z, Z}^{-1} \Theta_{Z, \{j,k\}}
\]

Given this, which of the following must be true for \( X_j \) and \( X_k \) to be \textbf{conditionally independent given \( Z \)}?

\begin{choices}
    \choice The determinant of \( \Theta_{(j,k) \mid Z} \) is zero.
    \choice The off-diagonal entry of \( \Theta_{(j,k) \mid Z} \) (i.e., \( (\Theta_{(j,k) \mid Z})_{jk} \)) is zero.
    \choice The sum of all precision matrix entries is zero.
    \choice The covariance matrix entry \( \Sigma_{jk} \) must be zero.
\end{choices}

\part \textbf{Step 4: Relating This to the Original Precision Matrix}

From our result, we see that the conditional independence condition is met when:

\begin{choices}
    \choice \( \Theta_{jk} = 0 \).
    \choice \( \Sigma_{jk} = 0 \).
    \choice \( \Theta^{-1}_{jk} = 0 \).
    \choice \( \Theta_{jj} = 0 \).
\end{choices}

\part \textbf{Step 5: Interpreting the Graph Structure}

In an undirected Gaussian graphical model, an edge exists between two nodes \( X_j \) and \( X_k \) if and only if:

\begin{choices}
    \choice \( \Theta_{jk} \neq 0 \).
    \choice \( \Sigma_{jk} \neq 0 \).
    \choice \( \Theta^{-1}_{jk} \neq 0 \).
    \choice \( \Theta_{jj} = 0 \).
\end{choices}

\end{parts}

\begin{solution}
    \begin{parts}
        \part 
        \part 
        \part
        \part
        \part
    \end{parts}
\end{solution}




\question[20 points] \textbf{Introduction to Conditional Random Fields} \droppoints  

A \textbf{Conditional Random Field (CRF)} is an undirected graphical model that models a conditional probability distribution \( P(Y \mid X) \) instead of the joint distribution \( P(X, Y) \). This is particularly useful for structured prediction problems where labels \( Y = (Y_1, ..., Y_n) \) depend on input features \( X = (X_1, ..., X_n) \).  

Consider a linear-chain CRF with sequence labels \( Y_1, Y_2, Y_3 \) conditioned on observations \( X_1, X_2, X_3 \). The probability of a labeling is given by:
\[
P(Y \mid X) = \frac{1}{Z(X)} \prod_{i=1}^{3} \psi(Y_i, Y_{i+1}, X)
\]
where:
\begin{itemize}
    \item \( \psi(Y_i, Y_{i+1}, X) = \exp(\theta_{Y_i, Y_{i+1}} + \sum_k w_k f_k(Y_i, X)) \)
    \item \( Z(X) \) is the partition function ensuring the probability distribution normalizes properly.
\end{itemize}

\begin{parts}
\part \textbf{Understanding Factorization}
What is the key difference between a CRF and a Markov Random Field (MRF)?
\begin{choices}
    \choice CRFs model \( P(Y \mid X) \), whereas MRFs model \( P(X, Y) \).
    \choice CRFs use directed edges, while MRFs use undirected edges.
    \choice CRFs require a fully connected graph, whereas MRFs do not.
    \choice MRFs only allow discrete variables, while CRFs allow continuous ones.
\end{choices}

\part \textbf{Computing Conditional Probabilities}
Suppose we are given parameter values \( \theta_{Y_i, Y_{i+1}} \) and feature weights \( w_k \). Which of the following is true about the conditional probability \( P(Y \mid X) \)?
\begin{choices}
    \choice It is computed by normalizing the product of potential functions over all possible label sequences.
    \choice It can be computed directly without using the partition function.
    \choice It is always independent of the input features \( X \).
    \choice It is equal to the sum of all local potential functions.
\end{choices}

\part \textbf{Parameter Estimation}
What is the most common method for learning CRF parameters \( \theta \) and \( w \) from labeled data?
\begin{choices}
    \choice Maximum Likelihood Estimation (MLE) via gradient descent.
    \choice Expectation-Maximization (EM) since CRFs have latent variables.
    \choice Bayesian inference using Gibbs Sampling.
    \choice k-Nearest Neighbors since CRFs are nearest-neighbor models.
\end{choices}
\end{parts}

\begin{solution}
    \begin{parts}
        \part  
        \part  
        \part  
    \end{parts}
\end{solution}


\question[20 points] \textbf{Introduction to Importance Sampling} \droppoints  

Importance sampling is a method for estimating expectations of a function \( f(x) \) under a distribution \( p(x) \), when direct sampling from \( p(x) \) is difficult. Instead, we sample from an easier proposal distribution \( q(x) \) and use importance weights to correct for the difference. We define:

\[
\mathbb{E}_{p(x)}[f(x)] = \int f(x) p(x) dx = \int f(x) \frac{p(x)}{q(x)} q(x) dx
\]

and approximate this expectation using \( L \) samples \( x^{(1)}, \dots, x^{(L)} \sim q(x) \):

\[
\mathbb{E}_{p(x)}[f(x)] \approx \frac{1}{\sum_{i=1}^L u_i} \sum_{i=1}^L f(x^{(i)}) u_i,
\]

where the unnormalized importance weights are:

\[
u_i = \frac{p(x^{(i)})}{q(x^{(i)})}.
\]

\begin{parts}
\part \textbf{Why Use Importance Sampling?}
Which of the following best describes why importance sampling is useful?
\begin{choices}
    \choice It allows us to estimate expectations when direct sampling from \( p(x) \) is difficult.
    \choice It generates exact samples from \( p(x) \) without error.
    \choice It reduces variance compared to sampling directly from \( p(x) \).
    \choice It removes the need to compute the normalizing constant of \( p(x) \).
\end{choices}

\part \textbf{Understanding Importance Weights}  
What can we say about the expected value of the importance weight \( u_i \) under the proposal distribution \( q(x) \)?

\begin{choices}
    \choice \( \mathbb{E}_{q(x)}[u_i] = \mathbb{E}_{p(x)}[u_i] \), since importance weights correct for sampling bias.
    \choice \( \mathbb{E}_{q(x)}[u_i] = 0 \) if \( p(x) \neq q(x) \).  
    \choice \( \mathbb{E}_{q(x)}[u_i] = 1 \), ensuring the estimator remains unbiased.  
    \choice \( \mathbb{E}_{q(x)}[u_i] \) grows as the difference between \( p(x) \) and \( q(x) \) increases.  
\end{choices}

\part \textbf{Variance of Importance Weights}  
What is the closed-form expression for the variance of the importance weights \( u_i = \frac{p(x)}{q(x)} \) under the proposal distribution \( q(x) \)?  

\begin{choices}
    \choice \( \text{Var}_{q(x)}[u_i] = \mathbb{E}_{q(x)}[u_i^2] - \mathbb{E}_{q(x)}[u_i]^2 \).
    \choice \( \text{Var}_{q(x)}[u_i] = \mathbb{E}_{p(x)}[u_i] - 1 \).
    \choice \( \text{Var}_{q(x)}[u_i] = \mathbb{E}_{q(x)}[p(x)] - \mathbb{E}_{q(x)}[q(x)] \).
    \choice \( \text{Var}_{q(x)}[u_i] = \frac{\mathbb{E}_{p(x)}[u_i^2]}{\mathbb{E}_{q(x)}[u_i]} \).
\end{choices}

\part \textbf{Variance of Importance Weights and High-Dimensional Spaces}  
Suppose that \( p(x) \) and \( q(x) \) are both factored distributions over \( d \) independent dimensions:
\[
p(x) = \prod_{i=1}^{d} p_i(x_i), \quad q(x) = \prod_{i=1}^{d} q_i(x_i).
\]
How does the variance of importance weights scale with \( d \), assuming that \( p_i(x_i)/q_i(x_i) \) has variance \( v \) for each individual dimension?

\begin{choices}
    \choice The variance remains constant as \( d \) increases.  
    \choice The variance scales linearly as \( \mathcal{O}(d) \).  
    \choice The variance scales exponentially as \( \mathcal{O}(v^d) \), where \( v \) is the per-dimension variance of importance weights.
    \choice The variance decreases as \( d \) increases due to averaging effects.  
\end{choices}
\end{parts}

\begin{solution}
    \begin{parts}
        \part  
        \part  
        \part  
        \part  
    \end{parts}
\end{solution}


\question[0 points (OPTIONAL)] \textbf{Parameter Learning in HMMs}\droppoints 
For those of you who want to start flexing your programming muscles, please enjoy implementing this E-M algorithm. This question is OPTIONAL and will not contribute to your grade.

\begin{figure}[h]
\centering
\begin{tikzcd}
Y_1 \arrow[r] \arrow[d] & Y_2 \arrow[r] \arrow[d] & \cdots \arrow[r] & Y_T \arrow[d] \\
X_1  &  X_2  & & X_T
\end{tikzcd}
\end{figure}

Consider an HMM with $ Y_t \in [M] $, $ X_t \in \mathbb{R}^{K} $ ($ M, K \in \mathbb{N} $).
Let $ (\pi, A, \{\mu_i, \sigma_i^2\}_{i=1}^M) $ be its parameters, where $ \pi \in \mathbb{R}^{M} $ is the initial state distribution, $ A \in \mathbb{R}^{M \times M} $ is the transition matrix, $ \mu_i \in \mathbb{R}^{K} $ and $ \sigma_i^2 > 0 $ are parameters of the emission distribution, which is defined to be an isotropic Gaussian. 
In other words,
\begin{align}
P(Y_1 = i) & = \pi_{i} \\
P(Y_{t+1} = j | Y_t = i) & = A_{ij} \\
P(X_t | Y_t = i) & = \Ncal(X_t; \mu_i, \sigma_i^2 I).
\end{align}


In the attached \verb|baum_welch.py| file, implement the Baum-Welch (EM) algorithm that estimates parameters from data $ \boldsymbol{X} \in \mathbb{R}^{N \times T \times K} $, which is a collection of $ N $ observed sequences of length $ T $. 
Please find unimplemented TODO blocks in the template for you to implement.
The template has its own toy problem to verify the implementation. 


\end{questions}
\end{document}