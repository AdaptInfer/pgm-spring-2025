---
layout: distill
title: Lecture 16 - Deep Learning from a GM Perspective
description: Deep learning foundations viewed through the lens of graphical models, covering perceptrons, neural networks, backpropagation, and probabilistic interpretations.
date: 2025-04-01

Lecturers:
  - name: Ben Lengerich
    url: "https://lengerichlab.github.io/"

authors:
  - name: Steven E. Haworth
  - name: Ying Chen

abstract: >
  Deep learning foundations viewed through the lens of graphical models, covering perceptrons, neural networks, backpropagation, and probabilistic interpretations.
date: 2025-04-01

---



## Logistics Review

- **Class webpage**: [lengerichlab.github.io/pgm-spring-2025](https://lengerichlab.github.io/pgm-spring-2025)
- [Lecture scribe sign-up sheet](https://docs.google.com/spreadsheets/d/1-Mj0MwkSxidVe-HfnMZyUIk4N8cwMeuGzEYTrgDjKqk/edit?gid=0)
- **Readings,Class Announcements, Assignment Submissions**: [Canvas](https://canvas.wisc.edu/courses/447453)
- **Instructor**: Ben Lengerich
  - Office Hours: Thursday 3:30-4:30pm, 7278 Medical Sciences Center
  - Email: [lengerich@wisc.edu](mailto:lengerich@wisc.edu)
- **TA**: Chenyang Jiang
  - Office Hours: Monday 11am-12pm, 1219 Medical Sciences Center
  - Email: [cjiang77@wisc.edu](mailto:cjiang77@wisc.edu)
- **Module 3**: Suggested focusing more on papers (or youtube vedios)

## Graded material

- Exam (20%, grades TBD)
- Project midterm report (5%, 4/11)
-  Project presentation (5%, 4/31, 5/1)
  - [Sign up here!](https://docs.google.com/spreadsheets/d/1ZRhn7_ESWGQRcdXahAdlHdoAW1gGG5UZbM98teQQpfY/edit?gid=0#gid=0)
- Project final report (15%, 5/5) 
- Extra credit (3%, [sign-up](https://docs.google.com/spreadsheets/d/1-Mj0MwkSxidVe-HfnMZyUIk4N8cwMeuGzEYTrgDjKqk/edit?gid=0#gid=0))

## About Research Papers

While research papers may appear rigorous and comprehensive, they often omit practical nuances. As learners, we should approach them optimistically — extracting value even from imperfect sources.

---

## From Biological to Artificial Neurons

Early AI models were inspired by biological neurons. The **McCulloch & Pitts** neuron, proposed in 1943, used threshold logic to compute simple Boolean functions like AND and OR. However, it **could not represent XOR**.

---

## The Perceptron and Its Limits

<figure id="perceptron" class="l-body-outset">
<div class="row">
  <div class="col three">
    <img src="{{ '/assets/img/notes/lecture-15/perceptron.png'| relative_url }}" 
         style="width:80%; max-width:800px;" />
  </div>
</div>
</figure>

The perceptron extended MP neurons by introducing **weighted inputs** and **activation functions** (like sigmoid):
$$
f(x) = \sigma(w^\top x + b)
$$
This model supports gradient-based learning for functions like:

$$
Y \sim \mathcal{N}(f(x), \Sigma) \Rightarrow \arg\min_w \sum_i \left(y_i - f(x_i; w)\right)^2
$$

### Why XOR Cannot Be Represented

Suppose XOR could be represented by a single-layer perceptron. Then:

- \( \sigma(w_1 + w_2) < \theta \) for input (1,1)
- \( \sigma(w_1) \geq \theta \), \( \sigma(w_2) \geq \theta \) for inputs (1,0), (0,1)

Adding the latter two contradicts the first — **no linear decision boundary exists**.

---

## Multi-Layer Perceptrons (MLP)

<figure id="MLP" class="l-body-outset">
<div class="row">
  <div class="col three">
    <img src="{{ '/assets/img/notes/lecture-15/NN.png'| relative_url }}" 
         style="width:75%; max-width:800px;" />
  </div>
</div>
</figure>


To model non-linear functions like XOR, **multi-layer perceptrons** are introduced:

- **Input Layer** \( x \)
- **Hidden Layer(s)**: \( h = \sigma(Wx + b) \)
- **Output Layer**: \( \hat{y} = W' h + b' \)

This architecture allows **hierarchical feature extraction** and can represent any continuous function under mild assumptions.

---

## Backpropagation

Neural networks are compositions of differentiable functions:

$$
L = \ell(f_3(f_2(f_1(x))))
$$


The gradient is computed via **reverse-mode autodiff**:

$$
\frac{dL}{dx} = \frac{dL}{df_3} \cdot \frac{df_3}{df_2} \cdot \frac{df_2}{df_1} \cdot \frac{df_1}{dx}
$$
This process is the core of **backpropagation**, enabling scalable training of deep networks.

---

## Graphical Models vs. Deep Nets

| Graphical Models (GMs)        | Deep Neural Networks (DNNs)   |
| ----------------------------- | ----------------------------- |
| Probabilistic semantics       | Function approximation        |
| Explicit latent variables     | Learned intermediate features |
| Inference via message passing | Learning via SGD              |

While GMs offer **interpretability**, DNNs provide **flexibility and scalability**. Hybrid models aim to combine both strengths.

---

## Probabilistic Neural Nets

### Restricted Boltzmann Machines (RBM)

<figure id="RBM" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-15/RBM.png' | relative_url }}" 
           style="width:60%; max-width:800px;" />
    </div>
  </div>
  </figure>



An RBM is an undirected graphical model with visible \( v \) and hidden \( h \) units:

$$
P(v, h) \propto e^{-E(v, h)}, \quad E(v, h) = -v^\top W h - a^\top v - b^\top h
$$


RBMs are trained using **contrastive divergence**, and serve as building blocks for deeper models.

---

### Deep Belief Networks (DBN)

<figure id="DBN" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-15/DBN.png' | relative_url }}" 
           style="width:45%; max-width:800px;" />
    </div>
  </div>
  </figure>




DBNs stack multiple RBMs and apply **layer-wise pretraining**, followed by supervised fine-tuning. They offer a probabilistic view of deep learning, with each layer capturing increasingly abstract representations.

<figure id="pretraining" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-15/pretraining.png' | relative_url }}" 
           style="width:35%; max-width:800px;" />
    </div>
  </div>
  </figure>




---

## NNs and GMs: Complementary Perspectives

Neural nets and graphical models can enhance each other:

- [@graves2013speech]: combined RNNs with probabilistic modeling for speech recognition.
- [@collobert2011natural]: integrated word embeddings with CRFs for NLP.

---

## References

- Toosi, A. N., & others. *A review on AI history*. 2021. [Link](https://linkinghub.elsevier.com/retrieve/pii/S1556859821000535)
- Graves et al., 2013
- Collobert & Weston, 2011

























