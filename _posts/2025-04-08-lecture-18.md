---
layout: distill
title: Lecture 18 - Deep Generative Models
description: Introduction to Variational Autoencoders, Generative Adversarial Networks, and Diffusion Models.
date: 2025-04-08

lecturers:
  - name: Ben Lengerich
    url: "https://lengerichlab.github.io/"

authors:
  - name: Wenyu Dai
  - name: Oliver Max Hannagan

abstract: >
  Introduction to DGMs: Variational Autoencoders, Generative Adversarial Networks, and Diffusion Models.
---

## Logistics Review

- **Class webpage**: [lengerichlab.github.io/pgm-spring-2025](https://lengerichlab.github.io/pgm-spring-2025)
- **Instructor**: Ben Lengerich
  - Office Hours: Thursday 3:30-4:30pm, 7278 Medical Sciences Center
  - Email: [lengerich@wisc.edu](mailto:lengerich@wisc.edu)
- **TA**: Chenyang Jiang
  - Office Hours: Monday 11am-12pm, 1219 Medical Sciences Center
  - Email: [cjiang77@wisc.edu](mailto:cjiang77@wisc.edu)


## Deep Generative Models

### Recall Generative and Discriminative models

- Generative:
  
      1. model joint distribution P(X,Y)
      2. Observe X and Y. Learn P(X|Y) and P(Y).
      3. Calculate P(X) by integrating P(X,Y) over Y, and eventually gets P(Y|X)
  
  - Example: Naive Bayes
    
- Discriminative:
  
      1. model conditional distribution P(Y|X)
      2. Observe X and Y and learn P(Y|X)
  
  - Example: logistic regression

### What are Deep Generative Models?

- Deep means many layers: $Z_{1}\to ...\to Z_{k}\to X$
- Define probablistic distributions over a set of variables.

## Early Forms of DGMs:

<figure id="sigmoidBelief" class="l-body-outset">
<div class="row">
  <div class="col three">
    <img src="{{ '/assets/img/notes/lecture-17/sigmoid_belief_net.png'| relative_url }}" 
         style="width:80%; max-width:800px;" />
  </div>
</div>
</figure>
A Probablistic nerual network that uses sigmoid activation functiona to model conditional probabilities. It uses directed edges where nodes are consisted of binary values. 

<figure id="helmholtz_machine" class="l-body-outset">
<div class="row">
  <div class="col three">
    <img src="{{ '/assets/img/notes/lecture-17/helmholtz_machine.png'| relative_url }}" 
         style="width:80%; max-width:800px;" />
  </div>
</div>
</figure>
Helmholtz machine has two networks as seen in the graph above. One is bottom-up that takes inputs and produces distributions over hidden layers. Another is top-down that generates values.

## How DGMs are trained?

- Via EM framework:
  - **E-step (Expectation):** Estimate the posterior distribution of the latent variables given the observed data.
  - **M-step (Maximization):** Maximize the expected log-likelihood with respect to model parameters.
    
- Sampling and Data Augmentation:
  - Ancestral sampling (for autoregressive models)
  - Gibbs sampling (for undirected models)
    
- Variational Inference:
  - Replace the true posterior with a simpler distribution.
  - Used in models like Variational Autoencoders.
  - Optimize the Evidence Lower Bound (ELBO):
    
$$
\log p(x) \geq \mathbb{E}_{q(z \mid x)}\left[ \log p(x \mid z) \right] - \mathrm{KL}\left( q(z \mid x) \,\|\, p(z) \right)
$$

- Wake and Sleep Algorithm:
  - **Wake phase:**
    - Use real data to update the generative model.
    - $E_{q(z|x)}[\log p(x|z)]$
    
  - **Sleep phase:**
      - Use generated data to train the inference model.
      - $E_{q(x|z)}[\log p(z|x)]$





