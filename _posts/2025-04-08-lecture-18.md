---
layout: distill
title: Lecture 18 - Deep Generative Models
description: Introduction to Variational Autoencoders, Generative Adversarial Networks, and Diffusion Models.
date: 2024-04-08

lecturers:
  - name: Ben Lengerich
    url: "https://lengerichlab.github.io/"

authors:
  - name: Wenyu Dai
  - name: Oliver Max Hannagan

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  Introduction to DGMs: Variational Autoencoders, Generative Adversarial Networks, and Diffusion Models.
---

## Logistics Review

- **Class webpage**: [lengerichlab.github.io/pgm-spring-2025](https://lengerichlab.github.io/pgm-spring-2025)
- **Instructor**: Ben Lengerich
  - Office Hours: Thursday 3:30-4:30pm, 7278 Medical Sciences Center
  - Email: [lengerich@wisc.edu](mailto:lengerich@wisc.edu)
- **TA**: Chenyang Jiang
  - Office Hours: Monday 11am-12pm, 1219 Medical Sciences Center
  - Email: [cjiang77@wisc.edu](mailto:cjiang77@wisc.edu)


## Deep Generative Models

### Recall Generative and Discriminative models
- Generative: model joint distribution P(X,Y)
  - Observe X and Y. Learn P(X|Y) and P(Y). Calculate P(X) by integrating P(X,Y) over Y, and eventually gets P(Y|X)
  - Example: Naive Bayes
- Discriminative: model conditional distribution P(Y|X)
  - Observe X and Y and learn P(Y|X)
  - Example: logistic regression

### What are Deep Generative Models?
- Deep means many layers: $Z_{1}\to ...\to Z_{k}\to X$
- Define probablistic distributions over a set of variables.

## Early Forms of DGMs:

<figure id="sigmoidBelief" class="l-body-outset">
<div class="row">
  <div class="col three">
    <img src="{{ '/assets/img/notes/lecture-17/sigmoid_belief_net.png'| relative_url }}" 
         style="width:80%; max-width:800px;" />
  </div>
</div>
</figure>
A Probablistic nerual network that uses sigmoid activation functiona to model conditional probabilities. It uses directed edges and nodes are consisted of binary values. 

<figure id="helmholtz_machine" class="l-body-outset">
<div class="row">
  <div class="col three">
    <img src="{{ '/assets/img/notes/lecture-17/helmholtz_machine.png'| relative_url }}" 
         style="width:80%; max-width:800px;" />
  </div>
</div>
</figure>
Helmholtz machine has two networks as seen in the graph above. One is bottom-up that takes inputs and produces distributions over hidden layers. Another is top-down that generates values.

## How DGMs are trained?














