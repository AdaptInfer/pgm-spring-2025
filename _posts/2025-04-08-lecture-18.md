---
layout: distill
title: Lecture 18 - Deep Generative Models
description: Introduction to Variational Autoencoders, Generative Adversarial Networks, and Diffusion Models.
date: 2025-04-08

lecturers:
  - name: Ben Lengerich
    url: "https://lengerichlab.github.io/"

authors:
  - name: Wenyu Dai
  - name: Oliver Max Hannagan
  - name: David Giardino

abstract: >
  Introduction to DGMs: Variational Autoencoders, Generative Adversarial Networks, and Diffusion Models.
---

## Logistics Review

- **Class webpage**: [lengerichlab.github.io/pgm-spring-2025](https://lengerichlab.github.io/pgm-spring-2025)
- **Instructor**: Ben Lengerich
  - Office Hours: Thursday 3:30-4:30pm, 7278 Medical Sciences Center
  - Email: [lengerich@wisc.edu](mailto:lengerich@wisc.edu)
- **TA**: Chenyang Jiang
  - Office Hours: Monday 11am-12pm, 1219 Medical Sciences Center
  - Email: [cjiang77@wisc.edu](mailto:cjiang77@wisc.edu)


## Deep Generative Models

### Recall Generative and Discriminative models

- Generative:
  
      1. model joint distribution P(X,Y)
      2. Observe X and Y. Learn P(X|Y) and P(Y).
      3. Calculate P(X) by integrating P(X,Y) over Y, and eventually gets P(Y|X)
  
  - Example: Naive Bayes
    
- Discriminative:
  
      1. model conditional distribution P(Y|X)
      2. Observe X and Y and learn P(Y|X)
  
  - Example: logistic regression

### What are Deep Generative Models?

- Deep means many layers: $Z_{1}\to ...\to Z_{k}\to X$
- Define probablistic distributions over a set of variables.

## Early Forms of DGMs:

<figure id="sigmoidBelief" class="l-body-outset">
<div class="row">
  <div class="col three">
    <img src="{{ '/assets/img/notes/lecture-17/sigmoid_belief_net.png'| relative_url }}" 
         style="width:80%; max-width:800px;" />
  </div>
</div>
</figure>

A Probablistic nerual network that uses sigmoid activation functiona to model conditional probabilities. It uses directed edges where nodes are consisted of binary values. 

<figure id="helmholtz_machine" class="l-body-outset">
<div class="row">
  <div class="col three">
    <img src="{{ '/assets/img/notes/lecture-17/helmholtz_machine.png'| relative_url }}" 
         style="width:80%; max-width:800px;" />
  </div>
</div>
</figure>

Helmholtz machine has two networks as seen in the graph above. One is bottom-up that takes inputs and produces distributions over hidden layers. Another is top-down that generates values.

## How DGMs are trained?

- Via EM framework:
  - **E-step (Expectation):** Estimate the posterior distribution of the latent variables given the observed data.
  - **M-step (Maximization):** Maximize the expected log-likelihood with respect to model parameters.
    
- Sampling and Data Augmentation:
  - Ancestral sampling (for autoregressive models)
  - Gibbs sampling (for undirected models)
    
- Variational Inference:
  - Replace the true posterior with a simpler distribution.
  - Used in models like Variational Autoencoders.
  - Optimize the Evidence Lower Bound (ELBO):
    
$$
\log p(x) \geq \mathbb{E}_{q(z \mid x)}\left[ \log p(x \mid z) \right] - \mathrm{KL}\left( q(z \mid x) \,\|\, p(z) \right)
$$

- Wake and Sleep Algorithm:
  - **Wake phase:**
    - Use real data to update the generative model.
    - $E_{q(z|x)}[\log p(x|z)]$
    
  - **Sleep phase:**
      - Use generated data to train the inference model.
      - $E_{q(x|z)}[\log p(z|x)]$


## Variational Autoencoders (VAEs)

### VAEs is variational inference plus autoencoders

Recall ELBO of VI where we let q(z|x) be some family that’s easier to optimize:

$$
\log p(x) \geq E_{z \sim q(z)}[\log p(x, z)] + H(q)
$$

Also, recall autoencoder:
  1. Use encoder to compress data into smaller details.
  2. Pass through latent space.
  3. Use decoder to recreate the original input.


<figure id="vae" class="l-body-outset">
<div class="row">
  <div class="col three">
    <img src="{{ '/assets/img/notes/lecture-17/vae.png'| relative_url }}" 
         style="width:80%; max-width:800px;" />
  </div>
</div>
</figure>

The idea here is simple. Autoencoder is not generative but we can make it generative by using variational inference.
We can use the inference model as encoder. Pass the generated data to letent space Z, and decode the data using generative model.

Now, we want to estimate the true parameter of θ of the generative model. The question is how to represent it?
1. We can chose a simple prior p(z) like normal distribution.
2. Then we can train the model by maximizing the likelehood of training data: $p_{\theta}(x) = \int_{}^{}p_{\theta}(z)p_{\theta}(x|z)dz$


### Reparameterization Trick  

To enable backpropagation through stochastic sampling, VAEs use the **reparameterization trick**:

$$
z = \mu(x) + \sigma(x) \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

This reformulation allows gradients to flow through \( \mu(x) \) and \( \sigma(x) \), making the sampling operation differentiable and trainable with gradient descent.

### Generating from a VAE  

After training, you can generate new data as follows:
1. Sample a latent vector \( z \sim p(z) \), often a standard Gaussian.
2. Pass \( z \) through the decoder to get \( x' \sim p(x|z) \).

This enables the model to create novel data that resembles the training distribution.


### GAN Architecture and Objective  
GANs consist of:
- A **Generator** \( G(z) \): maps random noise \( z \sim p(z) \) to data space, producing fake samples.
- A **Discriminator** \( D(x) \): attempts to distinguish real data from fake data.

The generator learns to fool the discriminator; the discriminator learns to detect the fakes. Training is formulated as a minimax game:

$$
\min_G \max_D \; \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p(z)}[\log(1 - D(G(z)))]
$$

This adversarial setup allows GANs to learn a rich, implicit distribution over data without explicitly modeling \( p(x|z) \).







