---
layout: distill
title: Lecture 04 - Conditional Independence and Directed GMs (BNs)
description: null
date: 2025-01-23

lecturers:
- name: Ben Lengerich
  url: https://lengerichlab.github.io/
  
authors:
- name: Amy Cai
- name: Nancy Liu

abstract: |
  A brief review on fundamental Statistics knowledges.
---

- [**Conditional Independence**](#conditional-independence)
- [**Naïve Bayes Classifier**](#naïve-bayes-classifier)
- [**Bayesian Networks (BNs)**](#bayesian-networks-bns)
- [**Hidden Markov Models (HMMs)**](#hidden-markov-models-hmms)


# Probabilistic Graphical Models: Core Concepts

---

## **Conditional Independence**

### **Definitions**
- **Independence**:  
  Variables $X$ and $Y$ are independent $(X \perp Y)$ if:
  
$$
P(X, Y) = P(X)P(Y)
$$

- **Conditional Independence**:  
  $X$ and $Y$ are conditionally independent given $Z$ if:

$$
P(X, Y \mid Z) = P(X \mid Z)P(Y \mid Z)
$$

<figure id="Figure for conditional independence" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/Conditional indenpendence.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

### **Example**
- **Medical Diagnosis**:  
  Let $X = \text{Fever}, Y = \text{Rash}, Z = \text{Measles}$.  
  If a patient has measles ($Z$), knowing they have a fever ($X$) provides no additional information about whether they develop a rash ($Y$).
<figure id="Example of conditional independence figure" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/Example of Conditional indenpendence.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

### **Relate to Naïve Bayes**
- Conditional independence allows us to compute $P(X|Y)$ efficiently.
- Switching the direction of one arrow does not change the probability.
- Switching the direction of **two** arrow changes the probability because there is a double-count evidence (two $X$'s repeatedly contains part of information about $Y$)
<figure id="Figure for Naïve Bayes (1)" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/Naïve Bayes(1).png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<figure id="Figure for Naïve Bayes (2)" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/Naïve Bayes(2).png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<figure id="Figure for Naïve Bayes (3)" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/Naïve Bayes(3).png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<figure id="Figure for double-count evidence" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/double-count evidence.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

---

## **Directed Graphical Models (causality relationship)**

### **1. Markov Chains**
- **Markov Property**:  
  The future state depends only on the current state:
   
$$
P(Z_{t+1} \mid Z_t, Z_{t-1}, \ldots) = P(Z_{t+1} \mid Z_t)
$$

- **Transition Matrix**:  
  Defines probabilities $P(Z_t \mid Z_{t-1})$.  
- **Application**:  
  Modeling sequences like weather patterns or stock prices.

<figure id="Figure for Markov Property" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/Directed PGM.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

### **2. Hidden Markov Models (HMMs)**
We need it when the underlying drivers are not observed
- **Components**:  
  - **Hidden States** ($Z_t$): Latent variables (e.g., emotional states in speech).  
  - **Observations** ($X_t$): Observed data (e.g., audio signals).  
  - **Transition Probability**: $P(Z_t \mid Z_{t-1})$.  
  - **Emission Probability**: $P(X_t \mid Z_t)$.  
- **Example**:  
  **Dishonest Casino**:  
  - Hidden states: Fair die ($Z=0$) vs. loaded die ($Z=1$).  
  - Observations: Dice rolls (e.g., $X=6$).  
  - Goal: Infer when the dealer switches dice based on observed rolls.
<figure id="Figure for HMM" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/Hidden Markov Model (HMM).png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>  

### **3. Bayesian Networks (BNs)**
- **Structure**:  
  Directed Acyclic Graph (DAG) where nodes represent variables and edges encode dependencies.  
- **Factorization**:  
  Joint distribution factorizes as:
  
$$
P(X_1, \ldots, X_n) = \prod_{i=1}^n P(X_i \mid \text{Parents}(X_i))
$$

- **Key Structures**:  
  - **Common Parent**:  
    $A \leftarrow B \rightarrow C$ ⟹ $A \perp C \mid B$.  
    *Example*: $B = \text{Season}, A = \text{Rain}, C = \text{Sprinkler}$.
<figure id="Figure for common parent" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/common parent.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>
  
  - **Cascade**:  
    $A \rightarrow B \rightarrow C $ ⟹ $ A \perp C \mid B$.  
    *Example*: $A = \text{Smoking}, B = \text{Lung Damage}, C = \text{Cough}$.
<figure id="Figure for cascade" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/Cascade.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>
  
  - **V-Structure (Collider)**:  
    $A \rightarrow B \leftarrow C$ ⟹ $A $ and $ C$ become dependent if $B$ is observed.  
    *Example*: $A = \text{Alarm}, B = \text{Burglary}, C = \text{Earthquake}$.
  <figure id="Figure for V-structure" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/V-structure.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

- **I-map**
  - **Independence set**: let $P$ be a distribution on $X$. Define $I(P)$ to be the set of independences $(X \perp Y \mid Z)$ that hold in $P$.
  - **I-Map**: Let $G$ be any graph object with an associated independence set $I(G)$. We say that $G$ is an **I-map** for an independent set $I$ if $I(G) \subseteq I$.
  - **I-Map Distribution**: We say $G$ is an I-map for $P$ if $G$ is an I-map for $I(P)$, when we use $I(G)$ as the associated independence set.
<figure id="Figure for I-map" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/I - Maps.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>
  
  - Facts: 
    - Any independence that $G$ asserts must hold in $P$. Conversely, $P$ could contain other independence that are not in $G$.
    - We are capable to use $G$ to estimate $P$.
    - Due to the property of BNs, $I(G)$ can be used to represent the local Markov assumption by $I_l(G)=\{X_i \perp NonDescendants_{X_i} \mid Pa_{X_i}:\forall i\}$
<figure id="Figure for NonDescendants" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/NonDescendants.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>
  
    - Example: $G_0$,$G_1$,$G_2$ can be I-map for $P_1$, while $G_0$ is not the I-map for $P_2$. Because $G$ implies independence that is not contained in $P_2$, so it cannot be used to estimate $P_2$.
    
<figure id="Figure for I-map example" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/I Map example.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

- **D-Separation**:  
  - A path between $X$ and $Y$ is **blocked** by $Z$ if:  
    1. **Chain** $\rightarrow \bullet \rightarrow$: Middle node is in $Z$.
    2. **Fork** $\leftarrow \bullet \rightarrow$: Middle node is in $Z$.  
    3. **Collider** $\rightarrow \bullet \leftarrow$: Middle node *and its descendants* are not in $Z$.
<figure id="Figure for chain" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/chain.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<figure id="Figure for Common Cause" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/Common Cause.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<figure id="Figure for collider" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/collider.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>
  
  - **Definition**: $X$ and $Y$ are d-separated given $Z$ if all paths are blocked.  
  - **MAG definition of D-Separation**: Variable $X$ and $Y$ are D-separated given $Z$ if they are separated in the moralized ancestral graph.
<figure id="Figure for D-separation" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/D-separation.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

  - **Example**:
<figure id="Figure for I(G) graph" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/I(G) graph.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

---

## **Learning & Inference**

### **Learning in BNs**
- **Parameter Learning**:  
  Estimate Conditional Probability Tables (CPTs) from data:
    
$$
P(X_i \mid \text{Parents}(X_i)) = \frac{\text{Count}(X_i, \text{Parents}(X_i))}{\text{Count}(\text{Parents}(X_i))}
$$

- **Structure Learning**:  
  Use algorithms like **K2** or **PC** to infer the DAG from data.  

### **Inference in BNs**
- **Exact Inference**:  
  - **Variable Elimination**: Marginalize variables step-by-step.  
  - **Junction Tree Algorithm**: Transform the BN into a tree structure.  
- **Approximate Inference**:  
  - **Sampling**: Markov Chain Monte Carlo (MCMC).  
  - **Loopy Belief Propagation**: Message-passing in cyclic graphs.  

---

## **I-Equivalence**

### **Definition**
- Two Bayesian Networks are **I-equivalent** if they encode the same set of conditional independence statements.  
- **Example**:  
  Networks $A \rightarrow B \rightarrow C$ and $A \leftarrow B \leftarrow C$ are I-equivalent.
<figure id="Figure for I-equivalence" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-04/Uniqueness of BNs.png' | relative_url }}" 
           style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

### **Implications**
- Different graph structures can represent identical independence relationships.  
- Critical for model selection and avoiding overfitting.  

---

## **Applications**

### **1. Naïve Bayes Classifier**
- **Assumption**: Features are conditionally independent given the class.  
- **Formula**:  

$$
P(Y \mid X_1, \ldots, X_n) \propto P(Y) \prod_{i=1}^n P(X_i \mid Y)
$$

- **Use Case**: Spam detection, sentiment analysis.  

### **2. Hidden Markov Models**
- **Applications**:  
  - Speech recognition (mapping audio to words).  
  - Bioinformatics (gene prediction from DNA sequences).  

### **3. Causal Inference**
- **Bayesian Networks for Causality**:  
  - Identify causal effects using interventions (e.g., $do(X=x)$).  
  - Example: Estimating the effect of a drug on recovery while controlling for confounders.  

---
